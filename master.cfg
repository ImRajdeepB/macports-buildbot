# -*- python -*-
# ex: set filetype=python:

import json
import os
import re

from buildbot.plugins import *
from buildbot.scheduler import Try_Userpass

# This is a sample buildmaster config file. It must be installed as
# 'master.cfg' in your buildmaster's base directory.


####### HELPER FUNCTIONS #######

def _path(name):
    return os.path.join(os.path.dirname(__file__), name)

def merge_dicts(*dicts):
    res = {}
    for d in dicts:
        res.update(d)
    return res

def port_from_path(path, sep='/'):
    components = path.split(sep)
    try:
        if (components[0] != '_resources' and components[2] in ('Portfile', 'files')):
            return components[1]
    except IndexError:
        pass
    # Might be better to throw a custom exception here?


# This is the dictionary that the buildmaster pays attention to. We also use
# a shorter alias to save typing.
c = BuildmasterConfig = {}

config = {
    # Production or development
    'production': False,

    # Connections
    'slaveport': 9989,
    'httpport': 8010,

    # External configuration. Use absolute paths when overriding these.
    'configfile': _path('config.json'),
    'secretsfile': _path('secrets.json'),
    'workersfile': _path('workers.json'),
    'htpasswdfile': _path('htpasswd'),

    # GitHub. Repository URLs must have the ".git" suffix.
    'githubsecretfile': _path('github.secret'),
    'baseurl': 'https://github.com/macports/macports-base.git',
    'mpbburl': 'https://github.com/macports/mpbb.git',
    'portsurl': 'https://github.com/macports/macports-ports.git',
    'wwwurl': 'https://github.com/macports/macports-www.git',
    'guideurl': 'https://github.com/macports/macports-guide.git',
    'infraurl': 'https://github.com/macports/macports-infrastructure.git',

    # Tooling
    'slaveprefix': '/opt/local',
    'toolsprefix': '/opt/mports',
    'jobstoolsprefix': '/opt/local',

    # Deployment
    'archivesite': 'http://packages.macports.org',
    'archivesiteprivate': 'https://packages-private.macports.org',
    'privkey': '',
    'deploy': {},

    # Site definitions
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#site-definitions)
    'title': 'MacPorts',
    'titleurl': 'https://www.macports.org/',
    'buildboturl': 'http://localhost:8010/',

    # Database
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#database-specification)
    'db_url': 'sqlite:///state.sqlite',

    # Data lifetime
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#data-lifetime)
    'buildhorizon': 10000,
    'cachedbuildrequests': 20000,
    'cachedbuilds': 600,
    'cachedchanges': 200,
    'eventhorizon': 2000,
    'loghorizon': 5000
    }

# Override defaults with external settings.
try:
    with open(config['configfile']) as f:
        extconfig = json.load(f)
except IOError:
    extconfig = {}
config.update(extconfig)

# Load secrets from external file
try:
    with open(config['secretsfile']) as f:
        secrets = json.load(f)
except IOError:
    secrets = {}

path_base = '/usr/bin:/bin:/usr/sbin:/sbin'
path_ports = os.path.join(config['toolsprefix'], 'bin') + ':' + path_base
path_jobs = os.path.join(config['jobstoolsprefix'], 'bin') + ':' + path_base


####### WORKERS

# The 'workers' list defines the set of recognized workers. Each element is
# a Worker object, specifying a unique worker name and password.  The same
# worker name and password must be configured on the worker.

with open('workers.json') as f:
    workerdata = json.load(f)

# convert unicode to byte strings
# build_platforms = [s.encode('utf-8') for s in workerdata['build_platforms']]
build_platforms = workerdata['build_platforms']

c['workers'] = []
# c['workers'] = [worker.Worker(name, pwd)
#                 for name, pwd in workerdata['workers'].items()]
c['workers'].append(worker.Worker("example-worker", "pass"))

# 'protocols' contains information about protocols which master will use for
# communicating with workers. You must define at least 'port' option that workers
# could connect to your master with this protocol.
# 'port' must match the value configured into the workers (with their
# --master option)
c['protocols'] = {'pb': {'port': 9989}}

####### CHANGESOURCES

# the 'change_source' setting tells the buildmaster how it should find out
# about source code changes.  Here we point to the buildbot version of a python hello-world project.

c['change_source'] = []
c['change_source'].append(changes.GitPoller(
        'https://github.com/macports/macports-ports.git',
        workdir='gitpoller-workdir', branch='master',
        pollInterval=300))


####### SCHEDULERS

# Configure the Schedulers, which decide how to react to incoming changes.  In this
# case, just kick off a 'runtests' build

base_platforms = [plat for plat in build_platforms if 'legacy' not in plat and '10.6_i386' not in plat]
port_platforms = [plat for plat in build_platforms if 'linux' not in plat and '10.5_ppc' != plat]

base_buildernames = list(map('base-{}'.format, base_platforms))
portwatcher_buildernames = list(map('ports-{}-watcher'.format, port_platforms))
portbuilder_buildernames = list(map('ports-{}-builder'.format, port_platforms))
portbuilder_triggerables = list(map('ports-{}-trigger'.format, port_platforms))

c['schedulers'] = []
# # The ChangeFilters assume that Git URLs end with ".git".
# c['schedulers'] = [
#     schedulers.SingleBranchScheduler(
#         name='base',
#         treeStableTimer=5,
#         change_filter=util.ChangeFilter(
#             repository=config['baseurl'][:-4],
#             branch_re='^(master|release-.*)$'),
#         builderNames=base_buildernames),
#     schedulers.SingleBranchScheduler(
#         name='ports',
#         # Don't start a separate build for every pushed commit.
#         treeStableTimer=5,
#         change_filter=util.ChangeFilter(
#             repository=config['portsurl'][:-4],
#             branch='master',
#             # Should actually skip changes to files/ only, but only if
#             # we know the last build of the port succeeded.
#             filter_fn=lambda change: any(port_from_path(f) for f in change.files)),
#         builderNames=portwatcher_buildernames),
#     schedulers.ForceScheduler(
#         name='base_force',
#         builderNames=base_buildernames),
# #    schedulers.ForceScheduler(
# #        name='portbuilder_force',
# #        builderNames=portbuilder_buildernames,
# #        properties=[util.StringParameter(
# #            name='portname',
# #            label='Port name:',
# #            default='',
# #            required=True)
# #        ]),
#     schedulers.ForceScheduler(
#         name='portwatcher_force',
#         builderNames=portwatcher_buildernames,
#         properties=[util.StringParameter(
#             name='portlist',
#             label='Port list:',
#             default='',
#             size=30,
#             required=True)])
#     ]

c['schedulers'].append(schedulers.SingleBranchScheduler(
                            name="all",
                            change_filter=util.ChangeFilter(branch='master'),
                            treeStableTimer=None,
                            builderNames=["runtests"]))
c['schedulers'].append(schedulers.ForceScheduler(
                            name="force",
                            builderNames=["runtests"]))
c['schedulers'].append(Try_Userpass(
    name='try',
    builderNames=['runtests'],
    port=5555,
    userpass=[('sampleuser', 'samplepass')]
))


####### BUILDERS

# The 'builders' list defines the Builders, which tell Buildbot how to perform a build:
# what steps, and which workers can execute them.  Note that any particular build will
# only take place on one worker.

factory = util.BuildFactory()
# check out the source
factory.addStep(steps.Git(
    repourl='https://github.com/macports/macports-ports.git',
    progress=True,
    mode='full',
    method='copy',
    env={'PATH': path_ports}))
factory.addStep(steps.Configure(command=util.WithProperties("""
env PATH=/usr/bin:/bin:/usr/sbin:/sbin ./configure --enable-readline \
    --prefix=%(workdir)s/opt/local \
    --with-applications-dir=%(workdir)s/opt/local/Applications \
    --with-install-user=`id -un` \
    --with-install-group=`id -gn` \
"""),logfiles={'config.log': 'config.log'}))
factory.addStep(steps.Compile(
    name='make -C vendor',
    command='make -j`sysctl -n hw.activecpu` -C vendor'))
factory.addStep(steps.Compile(
    name='make',
    command='make -j`sysctl -n hw.activecpu`'))
factory.addStep(steps.ShellCommand(
    command='make install',
    name='install',
    description=['installing'],
    descriptionDone=['install']))
factory.addStep(steps.ShellCommand(
    command='make test',
    name='test',
    description=['testing'],
    descriptionDone=['test']))
factory.addStep(steps.ShellCommand(
    command=util.WithProperties('make distclean; rm -rf %(workdir)s/opt/local'),
    name='clean',
    description=['cleaning'],
    descriptionDone=['clean']))
# run the tests (note that this will require that 'trial' is installed)
factory.addStep(steps.ShellCommand(command=["trial", "hello"],
                                   env={"PYTHONPATH": "."}))

# custom class to make the file list available on the slave...
class SetPropertyFromCommandWithPortlist(steps.SetPropertyFromCommand):
    def setBuild(self, build):
        super(SetPropertyFromCommandWithPortlist, self).setBuild(build)

        # support forced build properties
        ports = set(self.getProperty('portlist', default='').split())

        # paths should be category/portdir(/...)
        ports.update(ifilter(None, imap(port_from_path, self.build.allFiles())))

        self.setProperty('fullportlist', ' '.join(ports))

    def getText(self, cmd, results):
        if self.hasProperty('subportlist'):
            return ['Port list: {}'.format(self.getProperty('subportlist'))]
        # let ShellCommand describe
        return steps.ShellCommand.getText(self, cmd, results)

# can't run with prefix inside the workdir in production,
# because archives must be built with prefix=/opt/local
if config['production']:
    prefix = '/opt/local'
    dlhost = 'packages@packages-origin.macports.org'
    dlhost_private = dlhost
    dlpath = '/var/www/html/packages'
    dlpath_private = '/var/www/html/packages-private'
else:
    prefix = config['slaveprefix']
    dlhost = ''
    dlhost_private = dlhost
    dlpath = './deployed_archives'
    dlpath_private = './deployed_archives_private'

ulpath = 'archive_staging'
ulpath_unique = ulpath+'-%(buildername)s'

@util.renderer
def make_build_url(props):
    buildername = props.getProperty('buildername')
    buildnumber = props.getProperty('buildnumber')
    url = c['buildbotURL']
    if not url.endswith('/'):
        url += '/'
    url += 'builders/%s/builds/%s' % (buildername, buildnumber)
    return url

class TriggerWithPortlist(steps.Trigger):
    def getSchedulersAndProperties(self):
        sp = []
        priority = 1
        for scheduler in self.schedulerNames:
            for port in self.build.getProperty('subportlist').split():
                props = self.set_properties.copy()
                props['portname'] = port
                props['priority'] = priority
                priority += 1
                sp.append([scheduler, props])
        return sp



# -- Port Watcher --

def make_portwatcher_factory(triggerable):
    portwatcher_factory = util.BuildFactory()
    portwatcher_factory.useProgress = False
    portwatcher_factory.workdir = '../build'

    # get mpbb; we'll do the checkout of base and dports via these scripts
    portwatcher_factory.addStep(GitForTools(
        repourl=config['mpbburl'],
        progress=True,
        env={'PATH': path_ports},
        alwaysUseLatest=True,
        workdir=os.path.join(portwatcher_factory.workdir, 'mpbb'),
        haltOnFailure=True))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'cleanup'],
        name='cleanup',
        description=['cleaning'],
        descriptionDone=['clean']))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'selfupdate'],
        name='selfupdate',
        description=['updating', 'MacPorts'],
        descriptionDone=['update', 'MacPorts'],
        haltOnFailure=True))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'checkout',
                 '--archive-sites', config['archivesite']+' '+config['archivesiteprivate'],
                 '--binpath', '%(prefix)s/bin:%(prefix)s/sbin:%(toolsprefix)s/libexec/libarchive:/bin:/sbin:/usr/bin:/usr/sbin' % {'prefix': prefix, 'toolsprefix': config['toolsprefix']},
                 '--ports-url', config['portsurl']],
        timeout=3600,
        name='checkout',
        description=['syncing', 'ports'],
        descriptionDone=['sync', 'ports'],
        haltOnFailure=True))

    def extract_subportlist(rc, stdout, stderr):
        """
        Extract function for SetPropertyFromCommand(). Buildbot did not get the
        capability to ignore or distinguish stderr output before 0.9.x, but
        extract_fn always had the option to deal with them separately, so do
        that.
        This is called by SetPropertyFromCommand with the return value of the
        command and strings containing stdout and stderr. The return value
        should be a dictionary of new properties to be set.
        """
        if rc != 0:
            # Set an empty subport list on error
            return {'subportlist': ''}
        subports = [x.strip() for x in stdout.splitlines()]
        return {'subportlist': ' '.join(subports)}

    portwatcher_factory.addStep(SetPropertyFromCommandWithPortlist(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'list-subports',
                 '--archive-site', config['archivesite'],
                 '--archive-site-private', config['archivesiteprivate'],
                 util.WithProperties('%(fullportlist)s')],
        extract_fn=extract_subportlist,
        name='subports',
        description=['listing', 'subports'],
        haltOnFailure=True))

    def has_subportlist(step):
        return step.hasProperty('subportlist') and step.getProperty('subportlist')

    if 'mirror' in config['deploy']:
        portwatcher_factory.addStep(steps.Trigger(
            name='mirror',
            schedulerNames=['mirror'],
            set_properties={'subportlist': Property('subportlist'), 'triggered_by': make_build_url},
            waitForFinish=True,
            updateSourceStamp=True,
            doStepIf=has_subportlist))

    portwatcher_factory.addStep(TriggerWithPortlist(
        name='portbuilders',
        schedulerNames=[triggerable],
        set_properties={'triggered_by': make_build_url},
        waitForFinish=True,
        updateSourceStamp=True,
        doStepIf=has_subportlist))

    # make a logfile summarising the success/failure status for each port
    # (Current approach is not so useful as it is not incremental;
    #  ideally this would already be displayed during the Trigger step.)
    portwatcher_factory.addStep(steps.ShellCommand(
        command=['cat', os.path.join(logdir, 'ports-progress.txt')],
        name='summary',
        description=['summary']))

    return portwatcher_factory

# -- Port Builder --

portbuilder_factory = util.BuildFactory()
portbuilder_factory.useProgress = False
portbuilder_factory.workdir = '../build'
logdir = os.path.join(portbuilder_factory.workdir, 'logs')

portbuilder_factory.addStep(steps.Compile(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'install-dependencies', util.WithProperties('%(portname)s')],
    timeout=3600,
    name='install-dependencies',
    description=['installing', 'dependencies', 'of', util.WithProperties('%(portname)s')],
    descriptionDone=['install', 'dependencies', 'of', util.WithProperties('%(portname)s')],
    logfiles={'dependencies': os.path.join(logdir, 'dependencies-progress.txt')},
    haltOnFailure=True))

portbuilder_factory.addStep(steps.Compile(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'install-port', util.WithProperties('%(portname)s')],
    timeout=3600,
    name='install-port',
    description=['installing', util.WithProperties('%(portname)s')],
    descriptionDone=['install', util.WithProperties('%(portname)s')],
    logfiles={'files': os.path.join(logdir, 'port-contents.txt'),
              'statistics': os.path.join(logdir, 'port-statistics.txt'),
              'main.log': os.path.join(logdir, 'main.log')},
    haltOnFailure=True))

portbuilder_factory.addStep(steps.ShellCommand(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'gather-archives',
             '--archive-site', config['archivesite'],
             '--archive-site-private', config['archivesiteprivate'],
             '--staging-dir', ulpath],
    name='gather-archives',
    description=['gathering', 'archives'],
    descriptionDone=['gather', 'archives'],
    haltOnFailure=True))

# # upload archives from build slave to master
# portbuilder_factory.addStep(steps.DirectoryUpload(
#     slavesrc=ulpath,
#     masterdest=util.WithProperties(ulpath_unique)))

# XXX: move deploy_archives.sh functionality to mpbb
# sign generated binaries and sync to download server (if distributable)
if config['production']:
    portbuilder_factory.addStep(steps.MasterShellCommand(
        command=['./deploy_archives.sh', util.WithProperties(os.path.join(ulpath_unique, 'public'))],
        name='deploy-archives',
        description=['deploying', 'public', 'archives'],
        descriptionDone=['deploy', 'public', 'archives'],
        env={'PRIVKEY': config['privkey'], 'DLHOST': dlhost, 'DLPATH': dlpath}))
    portbuilder_factory.addStep(steps.MasterShellCommand(
        command=['./deploy_archives.sh', util.WithProperties(os.path.join(ulpath_unique, 'private'))],
        name='deploy-archives-private',
        description=['deploying', 'private', 'archives'],
        descriptionDone=['deploy', 'private', 'archives'],
        env={'PRIVKEY': config['privkey'], 'DLHOST': dlhost_private, 'DLPATH': dlpath_private}))

# TODO: do we want to upload the individual logs so maintainers can review them?
portbuilder_factory.addStep(steps.ShellCommand(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'cleanup'],
    name='cleanup',
    description=['cleaning'],
    descriptionDone=['clean'],
    alwaysRun=True))

class RsyncDeployStep(steps.ShellCommand):

    def __init__(self, host, user, srcpath, destpath, **kwargs):
        super(RsyncDeployStep, self).__init__(
                name='rsync',
                description=['deploying'],
                descriptionDone=['deploy'],
                command='rsync -avzhC --delay-updates --delete-delay %s %s@%s:%s/' % (srcpath, user, host, destpath),
                env={'RSYNC_RSH': 'ssh -i ssh_key -oUserKnownHostsFile=ssh_known_hosts'},
                **kwargs)

class PostgresDeployStep(steps.ShellCommand):

    def __init__(self, host, user, sqlfile, **kwargs):
        super(PostgresDeployStep, self).__init__(
                name='psql',
                description=['deploying'],
                descriptionDone=['deploy'],
                command='ssh -C -i ssh_key -oUserKnownHostsFile=ssh_known_hosts %s@%s psql < %s' % (user, host, sqlfile),
                **kwargs)


# def make_ssh_deploy_step(sshkeyfile, sshknownhostsfile, step):
#     return (
#         steps.FileDownload(
#             name='ssh key',
#             description=['transferring'],
#             descriptionDone=['transfer'],
#             mastersrc=sshkeyfile,
#             slavedest='ssh_key',
#             mode=0600),
#         steps.FileDownload(
#             name='ssh known_hosts',
#             description=['transferring'],
#             descriptionDone=['transfer'],
#             mastersrc=sshknownhostsfile,
#             slavedest='ssh_known_hosts',
#             mode=0600),
#         step)


class CDNPurgeStep(steps.MasterShellCommand):
    # XXX: Ensure py*-requests-oauthlib is available

    def __init__(self, zoneid, **kwargs):
        super(CDNPurgeStep, self).__init__(
                name='cdn',
                description=['purging', 'CDN zone'],
                descriptionDone=['purge', 'CDN zone'],
                command="./maxcdn-purge/maxcdn-purge.py %s %s" % (zoneid, config['deploy']['maxcdn']['secrets']),
                env={'PATH': "${PATH}"},
                **kwargs)


if 'www' in config['deploy']:
    jobs_www_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_www_factory.addStep(steps.Git(
        repourl=config['wwwurl'],
        progress=True,
        mode='full',
        method='copy',
        workdir='build/www',
        haltOnFailure=True))
    jobs_www_factory.addStep(steps.Compile(
        name='lint',
        description=['linting'],
        descriptionDone=['lint'],
        command='make lint',
        workdir='build/www'))
    jobs_www_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['www']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['www']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['www']['host'],
                user=config['deploy']['www']['user'],
                srcpath='www/',
                destpath=config['deploy']['www']['destpath'])))
    if 'maxcdn' in config['deploy'] and 'maxcdnzoneid' in config['deploy']['www']:
        jobs_www_factory.addStep(CDNPurgeStep(zoneid=config['deploy']['www']['maxcdnzoneid']))

if 'portindex' in config['deploy']:
    jobs_portindex_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_portindex_factory.addStep(GitForTools(
        name='git infrastructure',
        repourl=config['infraurl'],
        progress=True,
        alwaysUseLatest=True,
        mode='full',
        method='copy',
        workdir='build/infrastructure',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.Git(
        name='git ports',
        repourl=config['portsurl'],
        progress=True,
        alwaysUseLatest=True,
        mode='incremental',
        workdir='build/ports',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.ShellCommand(
        command='portindex',
        name='portindex',
        description=['indexing', 'ports'],
        descriptionDone=['index', 'ports'],
        workdir='build/ports',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.ShellCommand(
        command='port-tclsh infrastructure/jobs/portindex2postgres.tcl',
        name='portindex2postgres',
        description=['converting', 'index to SQL'],
        descriptionDone=['convert', 'index to SQL'],
        haltOnFailure=True))
    jobs_portindex_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['portindex']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['portindex']['sshknownhostsfile'],
            step=PostgresDeployStep(
                host=config['deploy']['portindex']['host'],
                user=config['deploy']['portindex']['user'],
                sqlfile='PortIndex.sql')))

if 'mirror' in config['deploy']:
    jobs_mirror_factory = util.BuildFactory()
    # This is the wrong absolute path! It's relative to the master; we want it relative to the worker.
    jobs_mirror_prefix = os.path.abspath(os.path.join(jobs_mirror_factory.workdir, 'prefix'))

    # get mpbb; we'll do the checkout of base and dports via these scripts
    jobs_mirror_factory.addStep(GitForTools(
        repourl=config['mpbburl'],
        progress=True,
        env={'PATH': path_jobs},
        alwaysUseLatest=True,
        workdir=os.path.join(jobs_mirror_factory.workdir, 'mpbb'),
        haltOnFailure=True))

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'selfupdate'],
        name='selfupdate',
        description=['updating', 'MacPorts'],
        descriptionDone=['update', 'MacPorts'],
        haltOnFailure=True))

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'checkout',
                 '--ports-url', config['portsurl']],
        timeout=3600,
        name='checkout',
        description=['syncing', 'ports'],
        descriptionDone=['sync', 'ports'],
        haltOnFailure=True))

    @util.renderer
    def make_jobs_mirror_command(props):
        cmd = ['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'mirror-distfiles', '--distfiles-dir', config['deploy']['mirror']['distfilesdir']]
        if props.hasProperty('subportlist') and props.getProperty('subportlist') is not None:
            cmd += props.getProperty('subportlist').split()
        return cmd

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=make_jobs_mirror_command,
        name='mirror',
        description=['mirroring', 'distfiles'],
        descriptionDone=['mirror', 'distfiles']))

if 'guide' in config['deploy']:
    jobs_guide_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_guide_factory.addStep(steps.Git(
        repourl=config['guideurl'],
        progress=True,
        mode='full',
        method='copy',
        workdir='build/guide',
        haltOnFailure=True))
    # TODO: check for existence of tools in toolsprefix
    jobs_guide_factory.addStep(steps.Compile(
        name='validate',
        description=['validating'],
        descriptionDone=['validate'],
        command='make validate',
        workdir='build/guide',
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_guide_factory.addStep(steps.Compile(
        command='make all',
        workdir='build/guide',
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_guide_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['guide']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['guide']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['guide']['host'],
                user=config['deploy']['guide']['user'],
                srcpath='guide/guide/html/',
                destpath=config['deploy']['guide']['destpath'])))
    if 'maxcdn' in config['deploy'] and 'maxcdnzoneid' in config['deploy']['guide']:
        jobs_guide_factory.addStep(CDNPurgeStep(zoneid=config['deploy']['guide']['maxcdnzoneid']))

if 'man' in config['deploy']:
    jobs_man_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_man_factory.addStep(steps.Git(
        repourl=config['baseurl'],
        progress=True,
        mode='full',
        method='copy',
        haltOnFailure=True))
    jobs_man_factory.addStep(steps.Configure(
        command="./standard_configure.sh",
        logfiles={'config.log': 'config.log'},
        haltOnFailure=True))
    jobs_man_factory.addStep(steps.Compile(
        command='make -C vendor/tcl/unix tclsh',
        name='make tclsh',
        description=['compiling', 'tclsh'],
        descriptionDone=['compile', 'tclsh'],
        warnOnWarnings=False,
        haltOnFailure=True))
    # TODO: check for existence of tools in toolsprefix
    jobs_man_factory.addStep(steps.ShellCommand(
        command='make -C doc html prefix=' + config['jobstoolsprefix'],
        name='make',
        description=['making', 'man pages'],
        descriptionDone=['make', 'man pages'],
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_man_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['man']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['man']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['man']['host'],
                user=config['deploy']['man']['user'],
                srcpath='doc/*.html',
                destpath=config['deploy']['man']['destpath'])))


####### BUILDER CONFIGURATION #######

def getPriority(request):
    if request.properties and request.properties.hasProperty('priority'):
        return int(request.properties.getProperty('priority'))
    else:
        return math.inf

def getNextBuildOnPortBuilder(builder, requests):
    nextBuild = requests[0]
    for request in requests:
        if getPriority(request) < getPriority(nextBuild):
            nextBuild = request
    return nextBuild

# XXX: slavenames assignment should be automatic and more generic
portsslaves = {}
baseslaves = {}
slavenames = workerdata['workers'].keys()
for plat in build_platforms:
    baseslaves[plat]  = filter(lambda x: x.endswith(plat+'-base'),  slavenames)
    portsslaves[plat] = filter(lambda x: x.endswith(plat+'-ports'), slavenames)

env_buildinfo = {
    'BUILDBOT_BUILDERNAME': util.WithProperties('%(buildername)s'),
    'BUILDBOT_BUILDNUMBER': util.WithProperties('%(buildnumber)s'),
    'BUILDBOT_BUILDURL': make_build_url
    }

c['builders'] = []
extract_os = re.compile(r'10\.\d+')

# for plat in build_platforms:
#     os_match = extract_os.search(plat)
#     os_version = os_match.group(0) if os_match else plat
#     if 'legacy' not in plat and '10.6_i386' not in plat:
#         c['builders'].append(
#             util.BuilderConfig(
#                 name='base-' + plat,
#                 workernames=['base-' + plat],
#                 factory=factory,
#                 tags=['base', os_version],
#                 env=merge_dicts(env_buildinfo, {'PATH': path_base})))
    # if 'linux' not in plat and '10.5_ppc' != plat:
    #     c['builders'].extend((
    #         util.BuilderConfig(
    #             name='ports-' + plat + '-watcher',
    #             workernames=['ports-' + plat],
    #             factory=make_portwatcher_factory('ports-' + plat + '-trigger'),
    #             tags=['portwatcher', os_version],
    #             env=merge_dicts(env_buildinfo, {'PATH': path_ports})),
    #         util.BuilderConfig(
    #             name='ports-' + plat + '-builder',
    #             workernames=['ports-' + plat],
    #             factory=portbuilder_factory,
    #             tags=['portbuilder', os_version],
    #             nextBuild=getNextBuildOnPortBuilder,
    #             env=merge_dicts(env_buildinfo, {'PATH': path_ports}))
    #         ))


if 'www' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-www',
            slavenames=['jobs'],
            factory=jobs_www_factory,
            tags=['jobs', 'docs', 'www'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'portindex' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-portindex',
            slavenames=['jobs'],
            factory=jobs_portindex_factory,
            tags=['jobs', 'portindex', 'www'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'guide' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-guide',
            slavenames=['jobs'],
            factory=jobs_guide_factory,
            tags=['jobs', 'docs', 'guide'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'man' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-man',
            slavenames=['jobs'],
            factory=jobs_man_factory,
            tags=['jobs', 'docs', 'man'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'mirror' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-mirror',
            slavenames=['jobs'],
            factory=jobs_mirror_factory,
            tags=['jobs', 'mirror', 'distfiles'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))

c['builders'].append(
    util.BuilderConfig(name="runtests",
      workernames=["example-worker"],
      factory=factory))
# c['builders'].append(
#     util.BuilderConfig(name="runagain",
#       workernames=["example-worker"],
#       factory=factory))


####### BUILDBOT SERVICES

# 'services' is a list of BuildbotService items like reporter targets. The
# status of each build will be pushed to these targets. buildbot/reporters/*.py
# has a variety to choose from, like IRC bots.

c['services'] = []
c['services'].append(reporters.IRC(host='irc.freenode.net', nick='bbtest',
                                   channels=['#buildbot-test']))


####### PROJECT IDENTITY

# the 'title' string will appear at the top of this buildbot installation's
# home pages (linked to the 'titleURL').

c['title'] = "MacPorts"
c['titleURL'] = "https://www.macports.org/"

# the 'buildbotURL' string should point to the location where the buildbot's
# internal web server is visible. This typically uses the port number set in
# the 'www' entry below, but with an externally-visible host name which the
# buildbot cannot figure out without some help.

c['buildbotURL'] = "http://localhost:8010/"

# minimalistic config to activate new web UI
c['www'] = dict(port=8010,
                plugins=dict(waterfall_view={}, console_view={}, grid_view={}, buildbot_vue_plugin_boilerplate={}))
c['www']['authz'] = util.Authz(
    allowRules = [
        util.AnyEndpointMatcher(role='admins')
    ],
    roleMatchers = [
        util.RolesFromUsername(roles=['admins'], usernames=['Alice'])
    ]
)
c['www']['auth'] = util.UserPasswordAuth([('Alice', 'Password1')])


####### DB URL

c['db'] = {
    # This specifies what database buildbot uses to store its state.  You can leave
    # this at its default for all but the largest installations.
    'db_url' : "sqlite:///state.sqlite",
}
